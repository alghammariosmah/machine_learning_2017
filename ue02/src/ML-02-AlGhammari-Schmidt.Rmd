---
title: "ML-02-AlGhammari-Schmidt"
author: "Bernd Schmidt"
date: "October 17, 2017"
output: pdf_document
---

```{r setup, message=FALSE, results='hide'}
require(plyr)
require(lattice)
require(ggplot2)
require(caret)
require(doMC)
library(pROC)
registerDoMC(3) 
```

# Part 1
*lecture script - no handin needed!*

# Part 2
## Dataset analysis

This part uses the *mtcars* dateset.
```{r 2_prepare}
data(mtcars)
# structure of mtcars
str(mtcars)
```

This dataset has no entry in a factor format therfore it must be converted.
There are only **42** samples in this dataset. Even though this is the answer to life, the universe and everything, when it comes to machine learning, it is the more the better.
Also, there are many features in comparison to the amount of samples.

```{r 2_analysis}
featurePlot(mtcars, factor(mtcars[,2]) , upper.panel=NULL, plot = 'density', plot.points=T, scales='free', auto.key=T)
pairs(mtcars[1:10], upper.panel=NULL, col=mtcars[,2])
```

It is visible that some features are not useable to distinguish the cylinder vlaues. Those are *vs*, *am* and *gear*.
Taking the other values in consideration, the combination of those values can be used to distinguish the values.

## Feature Analysis *hp* and *drat*

For analysis the features *hp* and *drat* were used. In the following figure the relation between those features can be seen.

```{r}
pairs(mtcars[,3:4], upper.panel=NULL, col=mtcars[,2])
```

It is visible, that the values for *cyl* form groups along a linear axis.
Therefor the solution will probably take three sections in which the values will belong to one of the groups for *cyl*.

Analysing the available samples in consideration of the *cyl* feature.

```{r 2_table}
table(factor(mtcars$cyl))
```

When using the cylinder as the Class of the dataset, it is visible, that there is an imbalance in the values.
We can upscale the vlaues to make an even distribution.

```{r 2_upscale}
cars <- upSample(x = mtcars[,-2], y = factor(mtcars[,2]), list = F)
table(cars$Class)
```

This upsampling will duplicate values to increase the number of entries to the biggest available.

## Train a KNN
Now a KNN can be trained, but without partitioning.

```{r 2_knn}
cars_knn <- train(x = cars[,2:3], y = cars$Class , method = 'knn',
                  tuneGrid = expand.grid(k=5), metric = 'Kappa', maximize = T,
                  trControl = trainControl(method = 'none', allowParallel = T)
)
# compute model
cars_knn
```


## Predict using KNN
Now the values for *cyl* can be predicted

```{r 2_predict}
predicted <- predict(cars_knn, newdata = cars[,2:3])
```

For further upcomings of the terms **TPR**, **TNR**, **FPR** and **FNR** following are the equations for those values:
$$ TPR = \frac{TP}{TP + FN} \\TNR = \frac{TN}{TN + FP} \\FPR = 1 - TPR \\FNR = 1 - TNR $$

```{r 2_conf}
confusionMatrix(data = predicted, reference = cars$Class)
```

The Kappa value in this case is **1**.
We have a TPR (or Sensitivity) of **1** and a TNR (or Specificity) of **1**.
Therefore the FPR is **0** as well as the FNR.

## KNN with probability
Now using this model to predict the values from the *mtcars* dataset.

```{r 2_predict_mtcars}
predicted_mtcars <-predict(cars_knn, newdata = mtcars[,3:4])
# building confusion matrix
confusionMatrix(data = predicted_mtcars, reference = mtcars[,2])
```

Also here, the cappa value is **1**.
We have a TPR (or Sensitivity) of **1** and a TNR (or Specificity) of **1**.
Therefore the FPR is **0** as well as the FNR.

But what happens to the trained model, if no upsampling was done?

```{r 2_no_upsample}
cars_knn_noup <- train(x = mtcars[,3:4], y = factor(mtcars[,2]) , method = 'knn',
                  tuneGrid = expand.grid(k=5), metric = 'Kappa', maximize = T,
                  trControl = trainControl(method = 'none', allowParallel = T)
)
# compute model
cars_knn_noup
# predict the values
predicted_noup <- predict(cars_knn_noup, newdata = mtcars[,3:4])
# displaying confusion matrix
confusionMatrix(data = predicted_noup, reference = mtcars[,2])
```

In difference to the values before, the Kappa value here is only **0.9508** (**1** before).
We have a TPR (or Sensitivity) of **0.8571** and a TNR (or Specificity) of **1**.
Therefore the FPR is **0.1429** and the FNR is **0**.

It is clearly stated, that upsampling, even though it just duplicates values, clearly improves the performance of the model.

Considering that there were only 14 samples used to compute the model, this model is most probably not very representing, because of the low amount of samples.

Now predicting the possibility of all samples using the ROC metric.

```{r 2_prop}
cars_factor = factor(paste0('c', cars$Class))
cars_knn_prop <- train(x = cars[,2:3], y = cars_factor , method = 'knn',
                  tuneGrid = expand.grid(k=5), metric = 'ROC', maximize = T,
                  trControl = trainControl(method = 'none', allowParallel = T,
                  classProbs = T)
)
# compute model
cars_knn_prop
# predict the values
predicted_prop <- predict(cars_knn_prop, newdata = cars[,2:3], type = 'prob')
```

```{r 2_roc}
roc_c4 <- roc(response = cars_factor=='c4', predictor = predicted_prop$c4)
roc_c6 <- roc(response = cars_factor=='c6', predictor = predicted_prop$c6)
roc_c8 <- roc(response = cars_factor=='c8', predictor = predicted_prop$c8)
# print information
{
  plot(roc_c4, col = 1)
  plot(roc_c6, col = 2, add = T)
  plot(roc_c8, col = 3, add = T)
  abline(0,1, col='gray70') # diagonal line for EER
  legend('bottomright', legend=c('c4', 'c6', 'c8'), lty=1, col=1:3, cex=0.9)
}
```

```{r 2_roc_values, tidy=TRUE}
roc_c4
roc_c6
roc_c8
```

## KNN with values k = 1 and k = 3 and lda
```{r 2_additional_models}
cars_knn_1 <- train(x = mtcars[,3:4], y = factor(mtcars[,2]) , method = 'knn',
                  tuneGrid = expand.grid(k=1), metric = 'Kappa', maximize = T,
                  trControl = trainControl(method = 'none', allowParallel = T)
)
# compute model
cars_knn_1

cars_knn_3 <- train(x = mtcars[,3:4], y = factor(mtcars[,2]) , method = 'knn',
                  tuneGrid = expand.grid(k=3), metric = 'Kappa', maximize = T,
                  trControl = trainControl(method = 'none', allowParallel = T)
)
# compute model
cars_knn_3

cars_lda <- train(x = mtcars[,3:4], y = factor(mtcars[,2]) , method = 'lda',
                  metric = 'RSME', maximize = T,
                  trControl = trainControl(method = 'none', allowParallel = T)
)
# compute model
cars_lda
```

```{r 2_model_eval}
# evaluate knn with k = 1
predicted <- predict(cars_knn_1, newdata=mtcars[,3:4] )
confusionMatrix(data = predicted, reference = mtcars[,2])
# evaluate knn with k = 3
predicted <- predict(cars_knn_3, newdata=mtcars[,3:4] )
confusionMatrix(data = predicted, reference = mtcars[,2])
# evaluate lda
predicted <- predict(cars_lda, newdata=mtcars[,3:4] )
confusionMatrix(data = predicted, reference = mtcars[,2])
```

It can be seen, that the values does not change over the used methods.
Most likely is that the values, the model was checked with, were no suitable values for a check as well as the model has been trained with very few samples.

# Part 3
Use the diamonds dataset delivered with the ggplot2 library:
```{r}
library(ggplot2)
data(diamonds)
```
Goal is to predict the diamond price from features carat, depth, table, and x:
What are your expectations from graphical data analysis?
It is a key using most of Machine Learning as it enables us to Understand the data where we will apply Machine Learning to. 

Are there features that might be useful?
Yes, features y and z could also be useful in prediction. 
```{r}
temp <- tail(diamonds[, c('price','carat','depth','table','x')],50) # taking the last 50 values for a faster plot
featurePlot(temp[,2:5], temp$price , col= 2:5, scales= list(relatio='free'), plot = 'pairs')
```

Use a KNN regression model and try different k, and a linear model (lm) or generalized linear model (glm), again with
using trainControl(method="none", ...):
* Use root mean squared error RMSE as performance metric.
* Hint: ensure a numeric target variable is handled to train.
```{r}

temp$price <- as.numeric(temp$price)
temp <- as.data.frame(unclass(temp)) # For some reason I had an error of "wrong model type for classification", so running this data frame helps  

modelLm <- train(x = temp[,2:5], 
                 y = temp[,1], # ensure this is numeric for regression and a factor for classification
                 method = 'lm', 
                 metric = 'RMSE', 
                 maximize = F, # minimize error measures, but maximize Accuracy, Kappa, ROC, etc
                 trControl = trainControl(method = 'none', allowParallel = T))

modelLm
```

Predict outcomes for your data and compute the RMSE and mean absolute error (MAE):

```{r}
predicted <- predict(modelLm, temp[,2:5])

# RMSE value on training data
sqrt(mean((predicted - temp[,1])**2)) 

# MAE value on training data
mean(abs(predicted - temp[,1]**2))
```

What is the error for these models? To give meaning to this error, state how big the error is compared to the
mean/median and standard deviation/mad of the diamond price. What are your thoughts about these results ("best"
model)?

The error is big as there is no stright line. So the prediction is not perfect. The mean and the median are the same for both predicted and real values. However, there are huge difference between the real and predicted values in both the standard deviation and MAD values.
```{r}
plot(predicted, temp[,1]) # plot predicted vs real values -- scatter represents error, straight line would mean perfect prediction

# The mean for real values:
mean(temp$price)
# The mean for predicted values:
mean(predicted)

# The Median for real values:
median(temp$price)
# The median for predicted values:
median(predicted)

# Standard Deviation for real values:
sqrt(var(temp$price))
# Standard Deviation for predicted values:
sqrt(var(predicted))

# MAD for real values:
mad(temp$price)
# MAD for real values:
mad(predicted)
```
 

# Part 4

Think about what the problem is with using the same data to train and evaluate a model. KNN with k=1 is an extreme
examples for this.

Explain the problem in your own words.
The problem with using the same data for training and evaluating the model is that the model can report the correct results every time it predicts the same data. It is like asking the model to predict the data that it has already seen before, or to predict the data that was used to create the model. 


What ways can you think of to prevent this problem?
For that problem, it is better to train in one set and evaluate on another set for better prediction. 
Regarding When K is small in knn, we are restraining the region of a given prediction and forcing our classifier to be “more blind” to the overall distribution. A small value for K provides the most flexible fit, which will have low bias but high variance. 







