---
title: "ML-05-AlGhammari-Schmidt"
author: "Bernd Schmidt , Osamah Al-Ghammari"
date: "November 03, 2017"
output: pdf_document
---

```{r setup, message=FALSE, results='hide'}
require(plyr)
require(lattice)
require(corrplot)
require(caret)
require(doMC)
registerDoMC(3)
library(zoo)
```

# Gesture Recognition

```{r read data}
filedir <- "../dat"
  
# get all filenames
filenames <- list.files(filedir, full.names = T, pattern="*.csv")

gestures <- list()

gestures$x <- ldply(filenames[1], read.table, sep=',', fill = T, col.names = c('gesture', 'person', 'sample', paste('acc', 1:1000, sep='')))
gestures$y <- ldply(filenames[2], read.table, sep=',', fill = T, col.names = c('gesture', 'person', 'sample', paste('acc', 1:1000, sep='')))
gestures$z <- ldply(filenames[3], read.table, sep=',', fill = T, col.names = c('gesture', 'person', 'sample', paste('acc', 1:1000, sep='')))

```

## Visualization

Currently the samples have different lengths and there are **NA** at the start and/or the end of the sample.

```{r visualize}
matplot(t(gestures$x[1:8,4:1003]), type='l', col=factor(gestures$x[1:8,1]))
```

## Optimization
For optimization, all **NA** values are removed and the values are interpolated to 1000 values per sample.
After that, a rolling median is applied to the sample to smooth it.

```{r optimize}
stepwidth <- 1/1000
optimize <- function(r) {
  row <- r[!is.na(r)] # remove all NA values
  row_approx <- approx(x = seq(0,1,1/(length(row[4:length(row)])-1)), y = row[4:length(row)], xout = seq(0,1,stepwidth), method = 'linear')$y # interpolate
  #row_runmed <- as.numeric(runmed(row_approx, k = 11)) # filter
  rollapply(row_approx, 30, median, na.rm=T)
  row_approx[1:1000]
}
gestures_opt <- list()
gestures_opt$x <- as.data.frame(t(apply(gestures$x, 1, optimize)))
gestures_opt$y <- as.data.frame(t(apply(gestures$y, 1, optimize)))
gestures_opt$z <- as.data.frame(t(apply(gestures$z, 1, optimize)))

gestures_opt$gesture <- gestures$x[,1]
gestures_opt$data <- gestures_opt$x
gestures_opt$data[,1001:2000] <- gestures_opt$y
gestures_opt$data[,2001:3000] <- gestures_opt$z
matplot(t(gestures_opt$data[1:8,1:1000]), type='l', col=factor(gestures_opt$gesture[1:8]))
```

## Data Validation and Optimization

```{r feature correlation}
# feature correlation as plot
corrplot(cor(gestures_opt$data[,4:100]), tl.cex = 0.3) # addgrid.col = NA
# remove correlated variable using ?findCorrelation
foundCorIndexes <- findCorrelation(cor(gestures_opt$data))
#foundCorIndexes
corrplot(cor(gestures_opt$data[,-foundCorIndexes]), tl.cex = 0.3)
# remove the features from the data
gestures_opt$data <- gestures_opt$data[,-foundCorIndexes]
```

## Data Partitioning

```{r data_partitioning}
# split into training and test data
set.seed(1704)
indexes_train <- createDataPartition(gestures_opt$gesture, p=0.75, list = F)
indexes_test <- (1:nrow(gestures_opt$data))[-indexes_train]

training <- gestures_opt$data[indexes_train,]
training_gest <- gestures_opt$gesture[indexes_train]
testing <- gestures_opt$data[indexes_test,]
testing_gest <- gestures_opt$gesture[indexes_test]
```

## Feature Selection

```{r feature_selection}
sbfRes <- sbf(x = training, y = training_gest, sbfControl = sbfControl(functions = rfSBF, method = 'repeatedcv', repeats = 5)) # more repeats are better
sbfRes
sbfRes$optVariables
gestures_opt$data <- gestures_opt$data[,sbfRes$optVariables]
```

## Model Training
Now we can use this data to train a model for detecting the gestures

```{r specify train control}
trControl <- trainControl(
                 method = 'LOOCV',
                 preProcOptions = list(thresh = 0.9),
                 returnResamp = 'final',
                 returnData = F,
                 savePredictions = T,
                 allowParallel = T
               )
```

### KNN

```{r model training knn}
models <- list()
models$knn <- train(training,
               factor(training_gest),
               method = 'knn',
               preProcess = c('center', 'scale', 'pca'),
               metric = 'Kappa',
               trControl = trControl
               )
models$knn
varImp(models$knn)
```

```{r predict_test_data knn}
predicted <- predict(models$knn, newdata = testing)

# to ensure, that also when one level is not predicted, the results can be displayed
u = union(predicted, testing_gest)
t = table(factor(predicted, u), factor(testing_gest, u))
conf <- confusionMatrix(t)

levelplot(sweep(conf$table, MARGIN = 2, STATS = colSums(conf$table), FUN = `/`), col.regions = gray(100:0/100))
```

### LDA
To compare the results, now a *lda* model with the same parameters is trained.

```{r model training lda}
models$lda <- train(training,
               factor(training_gest),
               method = 'lda',
               preProcess = c('center', 'scale', 'pca'),
               metric = 'Kappa',
               trControl = trControl
               )
models$lda
```

```{r predict_test_data lda}
predicted <- predict(models$lda, newdata = testing)

# to ensure, that also when one level is not predicted, the results can be displayed
u = union(predicted, testing_gest)
t = table(factor(predicted, u), factor(testing_gest, u))
conf <- confusionMatrix(t)

levelplot(sweep(conf$table, MARGIN = 2, STATS = colSums(conf$table), FUN = `/`), col.regions = gray(100:0/100))
```

### LDA2

```{r model training lda2}
models$lda2 <- train(training,
               factor(training_gest),
               method = 'lda2',
               preProcess = c('center', 'scale', 'pca'),
               metric = 'Kappa',
               trControl = trControl
               )
models$lda2
```

```{r predict_test_data lda2}
predicted <- predict(models$lda2, newdata = testing)

# to ensure, that also when one level is not predicted, the results can be displayed
u = union(predicted, testing$pers)
t = table(factor(predicted, u), factor(testing_gest, u))
conf <- confusionMatrix(t)

levelplot(sweep(conf$table, MARGIN = 2, STATS = colSums(conf$table), FUN = `/`), col.regions = gray(100:0/100))
```

### Neural Network

```{r train model neural network}
models$nn <- train(x = training,
           y = training_gest, # only use train data here!
           method = 'nnet', 
           metric = 'Kappa', 
           tuneGrid = expand.grid(size=9:17, decay=3**(-1:3)), # nnet parameters: size = #neurons in hidden layer, decay = weigth decay = regularization = overfitting protection
           trControl = trControl
)
```

```{r display nn}
print(models$nn)
plot(models$nn, scales = list(x = list(log = 3)))
levelplot(x = Kappa ~ size * decay, data = models$nn$results[models$nn$results$decay!=3 & models$nn$results$size != 1,], col.regions=gray(100:0/100), scales=list(y=list(log=3)))
# nnet plots: https://beckmw.wordpress.com/2013/11/14/visualizing-neural-networks-in-r-update/
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(models$nn$finalModel)
```

```{r predict_test_data nn}
predicted <- predict(models$nn, newdata = testing)

# to ensure, that also when one level is not predicted, the results can be displayed
u = union(predicted, testing$pers)
t = table(factor(predicted, u), factor(testing_gest, u))
conf <- confusionMatrix(t)

levelplot(sweep(conf$table, MARGIN = 2, STATS = colSums(conf$table), FUN = `/`), col.regions = gray(100:0/100))
```

### SVM

```{r train model svm}
train_model <- function(method, tuneGrid=NULL) {
  train(x = training, # in real life apps only use train data here!
        y = training_gest, # in real life apps only use train data here!
        method = method, 
        metric = 'Kappa', 
        tuneGrid = tuneGrid,
        trControl = trControl
  )
}
models$svmLinear <- train_model('svmLinear', tuneGrid = expand.grid(C=3**(-5:5)))
models$svmRadial <- train_model('svmRadial', tuneGrid = expand.grid(C=3**(-5:5), sigma=3**(-5:5)))
```

```{r display model svm}
print(plot(models$svmLinear, scales=list(x=list(log=3))))
print(plot(models$svmRadial, scales=list(x=list(log=3))))
```

```{r predict_test_data svm linear}
predicted <- predict(models$svmLinear, newdata = testing)

# to ensure, that also when one level is not predicted, the results can be displayed
u = union(predicted, testing$pers)
t = table(factor(predicted, u), factor(testing_gest, u))
conf <- confusionMatrix(t)

levelplot(sweep(conf$table, MARGIN = 2, STATS = colSums(conf$table), FUN = `/`), col.regions = gray(100:0/100))
```

```{r predict_test_data svm radial}
predicted <- predict(models$svmRadial, newdata = testing)

# to ensure, that also when one level is not predicted, the results can be displayed
u = union(predicted, testing$pers)
t = table(factor(predicted, u), factor(testing_gest, u))
conf <- confusionMatrix(t)

levelplot(sweep(conf$table, MARGIN = 2, STATS = colSums(conf$table), FUN = `/`), col.regions = gray(100:0/100))
```

## Result Comparision

```{r result comparison}
# save models to file to ensure that the results were not lost
saveRDS(object = models, file = "gesture_models.RDS")
results <- resamples(models)
summary(results)
bwplot(results)
```