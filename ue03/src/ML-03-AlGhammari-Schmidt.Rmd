---
title: "ML-02-AlGhammari-Schmidt"
author: "Bernd Schmidt , Osamah Al-Ghammari"
date: "October 17, 2017"
output: pdf_document
---

```{r setup, message=FALSE, results='hide'}
require(plyr)
require(lattice)
require(ggplot2)
require(kernlab)
require(caret)
require(doMC)
library(pROC)
registerDoMC(3) 
```

# Part 1

## Graphical analysis

Analysing the dateset *mtcars*.
```{r 1_analyse}
data(mtcars)
# showing the correlation of the values for distinguishing the number of cylinders
plot(mtcars$drat, mtcars$wt, col=mtcars$cyl)
# showing the densitiy of the cylinder values for each feature
featurePlot(mtcars[,5:6], factor(mtcars$cyl), upper.panel = NULL, plot = "density", plot.points = T, scales = 'free', auto.key = T)
```

Both plots show, that there is no clear seperation of the number of cylinders in the comined values from *wt* and *drat*.
Also the density shows, that the number of cylinders has no impact on the distribution of the features. On *drat* more than on *wt*.

But it is visible that the possibility to determine the cylinder value for 8 cylinders will be very high, because there are very less overlaps in the data.

```{r 2_no_parameter_svm}
mtcars_factor = factor(paste0('c', mtcars$cyl))
svm <- train( x = mtcars[,5:6],
              y = mtcars_factor,
              preProcess = NULL,
              method = "svmRadial",
              tuneGrid = expand.grid(sigma=30, C=10), # use DIFFERENT parameter ranges for your problems, e.g. try 3**(-10:10)
              metric = "Kappa",
              maximize = T,
              trControl = trainControl(
                method = 'none'
              )
            )
svm

predicted <- predict(svm, newdata = mtcars[,5:6])
confMatrix <- confusionMatrix(data = predicted, reference = mtcars_factor)
confMatrix
levelplot(sweep(x = confMatrix$table, STATS = colSums(confMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))
```

As previousely stated, the cylinder value for 8 cylinders was predicted 100% correct.
the other values were not predicted that good.
The problem, when using the same data for training and testing is that, the model already knew those values and therefore can predict them very well.
There are also just 32 samples in the dataset. Therefore the possibility of detecting this result on some real data is very low because there are millions of different cars in the world.

## Parameter Grid Search

```{r 2_parameter_svm}
tuneGrid <- expand.grid(C=3**(-5:5), sigma=3**(-3:3))
trControl <- trainControl(method = 'LOOCV', 
                          number = 10, 
                          returnData = F, 
                          classProbs = T, 
                          returnResamp = 'final', 
                          allowParallel = T)
mtcars_factor = factor(paste0('c', mtcars$cyl))
svm <- train( x = mtcars[,5:6],
              y = mtcars_factor,
              preProcess = NULL,
              method = "svmRadial",
              tuneGrid = tuneGrid,
              metric = "Kappa",
              maximize = T,
              trControl = trControl
            )
svm

plot(svm, scales=list(log=3))
# fitness landscape
levelplot(data = svm$results, x = Kappa ~ C * sigma, col.regions = gray(100:0/100), scales=list(log=3))
# final model parameters
svm$finalModel
# prediction
predicted <- predict(svm$finalModel, newdata = mtcars[,5:6])
confMatrix <- confusionMatrix(data = predicted, reference = mtcars_factor)
confMatrix
levelplot(sweep(x = confMatrix$table, STATS = colSums(confMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))
```

The **TPR** and **TNR** are both **1**, which means **all** values were predicted correct.
But in comparision to the model, the colleague/friend/classmate told us, the values for the SVM where very different.
$sigma=1$ and $C=243$.

This is caused to the automatic detection of parameters.
We don't test the model on different data. Therefore we cannot detect a overfitting in the model. In reference to the values from our final model, an overfitting is very likely.

# Part 2

## Your task is to use the ggplot2 diamonds dataset predict price from x only. Your are limited to using a regular linear model.

```{r}
library(ggplot2)
library(caret) 
library(doMC) 
registerDoMC(8) 

data(diamonds)

savedData <- as.data.frame(diamonds[, c('price','x')])

```


## What you you conclude from graphical data analysis?
It is noticeable that the price increases when the length of the diamond increases. 

```{r }

# I can't comprehend what the plot is about
plot(diamonds$x, col=diamonds$price, pch = 19)

```

## Train a generalized linear model glm using 10CV with 20 repeats. For this part of the assigment, not having an additional held-back test set and performing CV on all data is OK. When handing data to the x variable of caret::train, ensure that you hand it as a data frame (subsetting a data frame to a single variable can lead to it becoming a vector instead of a data frame, which will trigger errors in caret. You can e.g. use x=data.frame(YOUR_X), ... to obtain a data frame with 1 column from you vector)

```{r}
# train

model <- train(x = data.frame(savedData[,2]), 
               y = savedData[,1], # ensure this is a numeric for regression and a factor for classification
               method = 'glm', 
               trControl = trainControl(method = 'repeatedcv',
                                        number = 10,
                                        repeats = 20,
                                        allowParallel = T, 
                                        returnData = F,
                                        returnResamp = 'final',
                                        savePredictions = F,
                                        classProbs = F))
model$results

```

## Use the model to predict the target variable for all samples.

```{r }

trainPredicted <- predict(model, data.frame(savedData[,2]))


```


## State the distribution of the absolute error (e.g. summary(abs(predicted-observed))).
```{r }
summary(abs(trainPredicted-savedData[,1])**2)

```


## Visualize the predicted value over observed value in a scatterplot and add the ideal fit as diagonal line for reference.
```{r }
plot(trainPredicted, savedData[,1]) 
abline(0,1, col=2) 

```

## What do you see/derive from this plot?
The plot the predicted and the real values when using the general linear model. The scatter represents error, straight line would mean the ideal fit for the trained values with an error free model near 0. So, all predictions would be on this line



## Now train another linear model, but use a "trick" before training/evaluating the model: apply a logarithm to the target variable before.
```{r}
##################################################################
# data partitioning

library(caret) 
library(doMC) 
registerDoMC(8) 

set.seed(12345) # make it reproducible
indexes_train <- createDataPartition(savedData$price, p = 0.75, list = F) # indexes of training samples
indexes_test <- (1:nrow(savedData))[-indexes_train] # use all samples not in train as test samples

training <- as.data.frame(savedData[indexes_train,])
testing <- savedData[indexes_test,]


```

## Do graphical feature analysis before creating the model: what is different?
The price increases linearily with inceasing the length x. 

```{r}
featurePlot(training[,2], training$price)


```



## Again use 10CV with 20 repeats, then use the model to predict the target variable for all samples.
```{r}
model2 <- train(x = data.frame(training[,2]), 
               y = training[,1], 
               method = 'lm', 
               trControl = trainControl(method = 'repeatedcv',
                                        number = 10,
                                        repeats = 20,
                                        allowParallel = T, 
                                        returnData = F,
                                        returnResamp = 'final',
                                        savePredictions = F,
                                        classProbs = F))
model2
model2$results

trainPredicted2 <- predict(model2, data.frame(training[,2]))
summary(abs(trainPredicted2-training[,1])**2)


```

## Visualize the predicted values over observed values and add the ideal fit as line for reference to the plots: what is different to the situation before?

It is almost the same...
```{r}
plot(trainPredicted2, training[,1]) 
abline(0,1, col=2) 
```



# Part 3

Task is to predict the Class from these features. Split data into a 2% training and 98% test partition with set.seed(123456).
Use the training partition with 10CV with 20 repeats to train an LDA (lda) model (takes a while to compute). Compute the confusion matrix and Kappa for both the train and test partition. How do train and test errors differ? Keep the model object and training/test error/performance values for later comparison.

The best performance is reaching between 88% to 94% training set. 

##You might encounter warnings like Warning: In lda.default(x, grouping, ...) : variables are collinear or warning: prediction from a rank-deficient fit may be misleading when training the model. You can ignore these warnings for now. 
##Repeat the above for increased partition sizes (always using the same seed for partitioning!). Double the size of the training partition (4%, 8%, ...) until the gap between training and test error closes. Which train/test partition size do you need to close this gap? Keep all created models and metric values for later comparison.

```{r}
library(caret)
data(segmentationData)
str(segmentationData)
library(doMC) 
registerDoMC(8)


nums <- sapply(segmentationData, is.numeric) # Filtering only numeric values
numericData <- segmentationData[ , nums]

numericData$Class <- segmentationData$Class
col_idx <- grep("Class", names(numericData))
numericData <- numericData[, c(col_idx, (1:ncol(numericData))[-col_idx])] # moving Class to the first column

segmentationData <- numericData


set.seed(123456)
indexes_train <- createDataPartition(segmentationData$Class, p = 0.88, list = F) # indexes of training samples according to segmentationData[,1]
indexes_test <- (1:nrow(segmentationData))[-indexes_train] # use all samples not in train as test samples

training <- segmentationData[indexes_train,]
testing <- segmentationData[indexes_test,]

names(getModelInfo('lda')) # linear discriminant analysis 
getModelInfo('lda')[[1]]$parameters # ...with hyperparameters
modellda <- train(x = training[,2:60], 
                  y = training[,1], # ensure this is a numeric for regression and a factor for classification
                  preProcess = NULL, # center, scale, boxcox, pca, ...
                  method = 'lda', 
                  tuneGrid = NULL, 
                  metric = 'Kappa', 
                  maximize = T, 
                  trControl = trainControl(method = 'repeatedcv', 
                                           number = 10, # nr of CV partitions
                                           repeats = 20, 
                                           returnData = F, 
                                           classProbs = T,  
                                           returnResamp = 'final', 
                                           allowParallel = T))

modellda
modellda$results


# Confusion matrixse
# Training confusion Matrix
trainPredicted <- predict(modellda, newdata = training[,2:60])
trainConfMatrix <- confusionMatrix(data = trainPredicted, reference = training[,1]) 
trainConfMatrix
levelplot(sweep(x = trainConfMatrix$table, STATS = colSums(trainConfMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))

# Test confusion matrix
testPredicted <- predict(modellda, newdata = testing[,2:60])
testConfMatrix <- confusionMatrix(data = testPredicted, reference = testing[,1])
testConfMatrix
levelplot(sweep(x = testConfMatrix$table, STATS = colSums(testConfMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))


```
## Visualize the training and test performance for all created models in one plot (using the stored train and test set performance). A trend should be clearly visible in this plot




## Visualize the CV performance over all models in a second plot (e.g. boxplot). Again, a trend should be clearly visible here


## Given those models, which training and test partition size would you chose for your application scenario, and why? For the chosen model report a) the confusion matrix, TPR, TNR, FPR, and FNR, and b) visualize the ROC curve and state the AUC.

70% training set and 30% testing set could fit in such scenarios if I have multiple samples. With several samples the model will be able to predict better. 

## Think about what it means if your test data set becomes very small: what could be resulting implications?

Test data size is not as important as training set. Test set assess the performance of a fully-trained model.


# Part 4
##This is where we combine all previous techniques: data partitioning, parameter grid search, model training+evaluation and model selection in one classification task. Goal is to obtain the best suited model for a classification task.

##The task is to predict the Class from these features. Perform a 80/20 randomized train/test split, then use 10CV with 5 repeats (20 would be better, but for the sake of runtime, 5 is OK) to train different models (use the AUC ROC as metric to be optimized). For models using hyperparameters do an appropriate parameter grid search and visualize their performance over parameter values. You are free to try and experiment with any type of model, but try to understand what their concept is (see hints below). Keep in mind that some models are only applicable to regression (and some only to 2- class-classification, so they might work here, but not with other problems).

```{r}

library(caret)
data(segmentationData)
str(segmentationData)
library(pROC)
library(doMC) # paralellization
registerDoMC(8) # register 8 cores (more cores require more RAM)


# applied to the specific models 
trControl <- trainControl(method = 'repeatedcv', 
                          number = 10, 
                          repeats = 5, 
                          returnData = F, 
                          classProbs = T, 
                          summaryFunction = twoClassSummary,
                          returnResamp = 'final', 
                          allowParallel = T)


nums <- sapply(segmentationData, is.numeric) # Filtering only numeric values
numericData <- segmentationData[ , nums]

numericData$Class <- segmentationData$Class
col_idx <- grep("Class", names(numericData))
numericData <- numericData[, c(col_idx, (1:ncol(numericData))[-col_idx])] # moving Class to the first column

segmentationData <- numericData


set.seed(123456)
indexes_train <- createDataPartition(segmentationData$Class, p = 0.80, list = F) # indexes of training samples according to segmentationData[,1]
indexes_test <- (1:nrow(segmentationData))[-indexes_train] # use all samples not in train as test samples

training <- segmentationData[indexes_train,]
testing <- segmentationData[indexes_test,]

# aggregate all models in a list
models <- list()

models$modellda <- train(x = training[,2:60], 
                  y = training[,1], # ensure this is a numeric for regression and a factor for classification
                  preProcess = NULL, # center, scale, boxcox, pca, ...
                  method = 'lda', 
                  tuneGrid = NULL, 
                  metric = 'ROC', 
                  maximize = T, 
                  trControl = trControl)


models$modelKnn <- train( x = training[,2:60], 
                          y = training[,1], 
                          preProcess = NULL, 
                          method = 'knn', 
                          tuneGrid = expand.grid(k=1:10), 
                          metric = 'ROC', 
                          maximize = T, 
                          trControl = trControl)
#plot(models$modelKnn)


getModelInfo('svmLinear')[[1]]$parameters
models$modelsvmLinear <- train( x = training[,2:60], 
                          y = training[,1], 
                          preProcess = NULL, 
                          method = 'svmLinear', 
                          tuneGrid = NULL, #data.frame(.C = c(.25, .5, 1))
                          metric = "ROC", 
                          maximize = T, 
                          trControl = trControl)


models$modelglm <- train(x = training[,2:60], 
                         y = training[,1], 
                         preProcess = NULL, 
                         method = 'glm', 
                         tuneGrid = NULL, 
                         metric = "ROC", 
                         maximize = T, 
                         trControl = trControl)



```

## Compare model performances
```{r}
results <- resamples(models)
#summary(results)
bwplot(results) 


```

## Compare the CV confusion matrixes, ROC, and AUC of those models. You can e.g. plot the test ROC curve of all models into one figure to allow for an easy comparison

test Confusion matrix for LDA model:
```{r}
# test data model lda performance
testPredictedlda <- predict(models$modellda, testing[,2:60] )
testConfMatrix <- confusionMatrix(data = testPredictedlda, reference = testing[,1])
#testConfMatrix
levelplot(sweep(x = testConfMatrix$table, STATS = colSums(testConfMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))


```

test confusion matrix for KNN model:
```{r}

# test data model knn performance 
testPredictedknn <- predict(models$modelKnn, testing[,2:60]  )
testConfMatrix <- confusionMatrix(data = testPredictedknn, reference = testing[,1])
#testConfMatrix
levelplot(sweep(x = testConfMatrix$table, STATS = colSums(testConfMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))


```

test confusion matrix for Linear SVM
```{r}
# test data model Linear SVM performance 
testPredictedLinearSVM <- predict(models$modelsvmLinear, testing[,2:60]  )
testConfMatrix <- confusionMatrix(data = testPredictedLinearSVM, reference = testing[,1])
#testConfMatrix
levelplot(sweep(x = testConfMatrix$table, STATS = colSums(testConfMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))


```

test confusion matrix for GLM model
```{r}
# test data model GLM performance 
testPredictedglm <- predict(models$modelglm, testing[,2:60])
testConfMatrix <- confusionMatrix(data = testPredictedglm, reference = testing[,1])
#testConfMatrix
levelplot(sweep(x = testConfMatrix$table, STATS = colSums(testConfMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))
```

ROC for all models:
```{r}

testPredictedlda <- predict(models$modellda, testing[,2:60] , type = 'prob')
testPredictedknn <- predict(models$modelKnn, testing[,2:60]  , type = 'prob')
testPredictedLinearSVM <- predict(models$modelsvmLinear, testing[,2:60]  , type = 'prob')
testPredictedglm <- predict(models$modelglm, testing[,2:60]  , type = 'prob')

plot(roc(response = testing[,1],     predictor = testPredictedlda[,1]),     col = 1) 
plot(roc(response = testing[,1],   predictor = testPredictedknn[,1]), col = 2, add = T) 
plot(roc(response = testing[,1],  predictor = testPredictedLinearSVM[,1]),  col = 3, add = T)
plot(roc(response = testing[,1],  predictor = testPredictedglm[,1]),  col = 4, add = T)
abline(0,1, col='gray70') # diagonal line for EER
legend('bottomright', legend=c('lda', 'knn', 'LinearSVM', 'glm'), lty=1, col=1:3, cex=0.9)
```




 


# Part 5

## Guidelines

1. **Fundamental tenets of biometrics**
To make a clear statement about the accuracy of a biometric tenet, it must be evaluated in a large scale.
1. **Application domain**
An application must be kept in mind while doing research because otherwise the results can not be used in this application.
1. **Choice of biometric trait**
When performing research for new traits, all properties for the use in the target application must should be evaluated.
1. **Comparing biometric systems**
An increase only in accuracy can not be taken as a significant achievment.
1. **Comparing biometric systems**
When comparing algorithms, the statistical significance of this performance difference must be reported.
1. **Baseline**
Comparision of an algorithm against a known baseline is imperative.
1. **Evaluating biometric system components**
When developing new components, the components should be compared with identical input from a proper baseline and the same scoring function applied to the outputs.
1. **Choice of accuracy metric**
ROC and CMC curves should be reported together. Only if the target application operates ina closed-set environment, the CMC curve can be reported without the ROC curve. For open-set, FPIR and FNIR must be reported against each other in a plot.
1. **Choice of datasets**
A large and challenging dataset should be used, because in a large real world application, the chance of similar faces for example will increase with the number of enrolled users.
1. **Generalization across datasets**
The data should represent the target population and environment. Also the data should be captured in multiple sessions spanning over a period of time to increase the intra-clas variability.
1. **Training, validation, and test sets**
Those test sets should b non-overlapping to avoid positive bias in the results. This is because the target population is typically unknown, the databases are dynamic and lack a ground truth.
1. **Experimental protocol**
When publishing results, details about the experimental protocol and the characteristics of the dataset must be included. This is to make it possible to reproduce the results.
1. **Establishing the ground truth**
Ground truth must be reviewed for their correctness, because if there are some errors, the final correctness of the algorithm can not be lower than this error.
1. **Biometric fusion**
To increase the systems accuracy, multiple traits can be fused. But it must be weighed against the associated overhead. Therefore fusing traits like iris and face are prefereable.
1. **Vulnerabilities of a biometric system**
Vulnerabilities like presenting a spoof or altering the bimetric sample at the senor as well as tampering with the templates stored in the database must be detected or deflected. Also these countermeasures must assure that there is no significant loss in recognition accuracy.
