---
title: "ML-02-AlGhammari-Schmidt"
author: "Bernd Schmidt , Osamah Al-Ghammari"
date: "October 17, 2017"
output: pdf_document
---

```{r setup, message=FALSE, results='hide'}
#TODO: update required packages
require(plyr)
require(lattice)
require(ggplot2)
require(kernlab)
require(caret)
require(doMC)
library(pROC)
registerDoMC(3) 
```

# Part 1

## Graphical analysis

Analysing the dateset *mtcars*.
```{r 1_analyse}
data(mtcars)
# showing the correlation of the values for distinguishing the number of cylinders
plot(mtcars$drat, mtcars$wt, col=mtcars$cyl)
# showing the densitiy of the cylinder values for each feature
featurePlot(mtcars[,5:6], factor(mtcars$cyl), upper.panel = NULL, plot = "density", plot.points = T, scales = 'free', auto.key = T)
```

Both plots show, that there is no clear seperation of the number of cylinders in the comined values from *wt* and *drat*.
Also the density shows, that the number of cylinders has no impact on the distribution of the features. On *drat* more than on *wt*.

But it is visible that the possibility to determine the cylinder value for 8 cylinders will be very high, because there are very less overlaps in the data.

```{r 2_no_parameter_svm}
mtcars_factor = factor(paste0('c', mtcars$cyl))
svm <- train( x = mtcars[,5:6],
              y = mtcars_factor,
              preProcess = NULL,
              method = "svmRadial",
              tuneGrid = expand.grid(sigma=30, C=10), # use DIFFERENT parameter ranges for your problems, e.g. try 3**(-10:10)
              metric = "Kappa",
              maximize = T,
              trControl = trainControl(
                method = 'none'
              )
            )
svm

predicted <- predict(svm, newdata = mtcars[,5:6])
confMatrix <- confusionMatrix(data = predicted, reference = mtcars_factor)
confMatrix
levelplot(sweep(x = confMatrix$table, STATS = colSums(confMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))
```

As previousely stated, the cylinder value for 8 cylinders was predicted 100% correct.
the other values were not predicted that good.
The problem, when using the same data for training and testing is that, the model already knew those values and therefore can predict them very well.
There are also just 32 samples in the dataset. Therefore the possibility of detecting this result on some real data is very low because there are millions of different cars in the world.

## Parameter Grid Search

```{r 2_parameter_svm}
tuneGrid <- expand.grid(C=3**(-5:5), sigma=3**(-3:3))
trControl <- trainControl(method = 'LOOCV', 
                          number = 10, 
                          returnData = F, 
                          classProbs = T, 
                          returnResamp = 'final', 
                          allowParallel = T)
mtcars_factor = factor(paste0('c', mtcars$cyl))
svm <- train( x = mtcars[,5:6],
              y = mtcars_factor,
              preProcess = NULL,
              method = "svmRadial",
              tuneGrid = tuneGrid,
              metric = "Kappa",
              maximize = T,
              trControl = trControl
            )
svm

plot(svm, scales=list(log=3))
# fitness landscape
levelplot(data = svm$results, x = Kappa ~ C * sigma, col.regions = gray(100:0/100), scales=list(log=3))
# final model parameters
svm$finalModel
# prediction
predicted <- predict(svm$finalModel, newdata = mtcars[,5:6])
confMatrix <- confusionMatrix(data = predicted, reference = mtcars_factor)
confMatrix
levelplot(sweep(x = confMatrix$table, STATS = colSums(confMatrix$table), MARGIN = 2, FUN = '/'), col.regions=gray(100:0/100))
```

The **TPR** and **TNR** are both **1**, which means **all** values were predicted correct.
But in comparision to the model, the colleague/friend/classmate told us, the values for the SVM where very different.
$sigma=1$ and $C=243$.

This is caused to the automatic detection of parameters.
We don't test the model on different data. Therefore we cannot detect a overfitting in the model. In reference to the values from our final model, an overfitting is very likely.

## Part 2
Your task is to use the ggplot2 diamonds dataset predict price from x only. Your are limited to using a regular linear model.

```{r}
library(ggplot2)
library(caret) 
library(doMC) 
registerDoMC(8) 

data(diamonds)

savedData <- as.data.frame(diamonds[, c('price','x')])

```


# What you you conclude from graphical data analysis?
It is noticeable that the price increases when the length of the diamond increases. 

```{r }

# I can't comprehend what the plot is about
plot(diamonds$x, col=diamonds$price, pch = 19)

```

# Train a generalized linear model glm using 10CV with 20 repeats. For this part of the assigment, not having an additional held-back test set and performing CV on all data is OK. When handing data to the x variable of caret::train, ensure that you hand it as a data frame (subsetting a data frame to a single variable can lead to it becoming a vector instead of a data frame, which will trigger errors in caret. You can e.g. use x=data.frame(YOUR_X), ... to obtain a data frame with 1 column from you vector)

```{r}
# train

model <- train(x = data.frame(savedData[,2]), 
               y = savedData[,1], # ensure this is a numeric for regression and a factor for classification
               method = 'glm', 
               trControl = trainControl(method = 'repeatedcv',
                                        number = 10,
                                        repeats = 20,
                                        allowParallel = T, 
                                        returnData = F,
                                        returnResamp = 'final',
                                        savePredictions = F,
                                        classProbs = F))
model$results

```

# Use the model to predict the target variable for all samples.

```{r }

trainPredicted <- predict(model, data.frame(savedData[,2]))


```


# State the distribution of the absolute error (e.g. summary(abs(predicted-observed))).
```{r }
summary(abs(trainPredicted-savedData[,1])**2)

```


# Visualize the predicted value over observed value in a scatterplot and add the ideal fit as diagonal line for reference.
```{r }
plot(trainPredicted, savedData[,1]) 
abline(0,1, col=2) 

```

# What do you see/derive from this plot?
The plot the predicted and the real values when using the general linear model. The scatter represents error, straight line would mean the ideal fit for the trained values with an error free model near 0. So, all predictions would be on this line



# Now train another linear model, but use a "trick" before training/evaluating the model: apply a logarithm to the target variable before.
```{r}
##################################################################
# data partitioning

library(caret) 
library(doMC) 
registerDoMC(8) 

set.seed(12345) # make it reproducible
indexes_train <- createDataPartition(savedData$price, p = 0.75, list = F) # indexes of training samples
indexes_test <- (1:nrow(savedData))[-indexes_train] # use all samples not in train as test samples

training <- as.data.frame(savedData[indexes_train,])
testing <- savedData[indexes_test,]


```

# Do graphical feature analysis before creating the model: what is different?
The price increases linearily with inceasing the length x. 

```{r}
featurePlot(training[,2], training$price)


```



# Again use 10CV with 20 repeats, then use the model to predict the target variable for all samples.
```{r}
model2 <- train(x = data.frame(training[,2]), 
               y = training[,1], 
               method = 'lm', 
               trControl = trainControl(method = 'repeatedcv',
                                        number = 10,
                                        repeats = 20,
                                        allowParallel = T, 
                                        returnData = F,
                                        returnResamp = 'final',
                                        savePredictions = F,
                                        classProbs = F))
model2
model2$results

trainPredicted2 <- predict(model2, data.frame(training[,2]))
summary(abs(trainPredicted2-training[,1])**2)


```

# Visualize the predicted values over observed values and add the ideal fit as line for reference to the plots: what is different to the situation before?

It is almost the same...
```{r}
plot(trainPredicted2, training[,1]) 
abline(0,1, col=2) 
```



# Part 3

# Part 4

# Part 5